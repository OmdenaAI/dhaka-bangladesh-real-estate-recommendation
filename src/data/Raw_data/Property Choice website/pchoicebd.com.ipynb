{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28485c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284775f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246de95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read both CSV files\n",
    "df_rent = pd.read_csv('property_choice_website_data_for_rent_with_link.csv')\n",
    "df_sale = pd.read_csv('property_choice_web_data_for_sale_with_link.csv')\n",
    "\n",
    "# concatenate the two dataframes vertically and preserve column order\n",
    "df_combined = pd.concat([df_rent[['Link', 'Title', 'Price', 'Property ID', 'Area', 'Floor', 'Bedrooms', 'Bathrooms', 'Description', 'Latitude', 'Longitude', 'Features', 'Location', 'Status', 'Garages', 'Type']],\n",
    "                        df_sale[['Link', 'Title', 'Price', 'Property ID', 'Area', 'Floor', 'Bedrooms', 'Bathrooms', 'Description', 'Latitude', 'Longitude', 'Features', 'Location', 'Status', 'Garages', 'Type']]])\n",
    "\n",
    "# write the combined dataframe to a new CSV file\n",
    "df_combined.to_csv('property_choice_website_data_with_link.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f635fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90495a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fe301",
   "metadata": {},
   "source": [
    "## This code is for getting the links of the properties which are for rent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0373c7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property links saved to property_choice_web_links_rent.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "base_url = 'https://www.pchoicebd.com/property-status/for-rent/'\n",
    "property_links = []\n",
    "\n",
    "# Iterate through pages 1 to 21\n",
    "for page_number in range(1, 22):\n",
    "    if page_number == 1:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = f\"{base_url}page/{page_number}/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all article elements\n",
    "    articles = soup.find_all('article')\n",
    "\n",
    "    # Extract the links from the articles\n",
    "    for article in articles:\n",
    "        link = article.find('a', class_='btn-default')\n",
    "        if link:\n",
    "            property_links.append(link['href'])\n",
    "\n",
    "# Save the links to a CSV file\n",
    "with open('property_choice_web_links_rent.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['property_links'])\n",
    "\n",
    "    for link in property_links:\n",
    "        csv_writer.writerow([link])\n",
    "\n",
    "print(\"Property links saved to property_choice_web_links_rent.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be282a0",
   "metadata": {},
   "source": [
    "## This code is for getting the links of the properties which are for sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4402e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = 'https://www.pchoicebd.com/property-status/for-sale/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all article elements\n",
    "articles = soup.find_all('article')\n",
    "\n",
    "property_links = []\n",
    "\n",
    "# Extract the links from the articles\n",
    "for article in articles:\n",
    "    link = article.find('a', class_='btn-default')\n",
    "    if link:\n",
    "        property_links.append(link['href'])\n",
    "\n",
    "# Save the links to a CSV file\n",
    "with open('property_choice_web_links_sale.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['property_links'])\n",
    "\n",
    "    for link in property_links:\n",
    "        csv_writer.writerow([link])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62c5dc",
   "metadata": {},
   "source": [
    "## The code below gets the link of the properties for rent. This also rechecks how many properties are actually there. Because it says 1000+ in the home page but actually just 200+. Perhaps the others were deleted already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "810353cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: 47 articles found\n",
      "Page 2: 34 articles found\n",
      "Page 3: 20 articles found\n",
      "Page 4: 18 articles found\n",
      "Page 5: 7 articles found\n",
      "Page 6: 8 articles found\n",
      "Page 7: 8 articles found\n",
      "Page 8: 9 articles found\n",
      "Page 9: 7 articles found\n",
      "Page 10: 2 articles found\n",
      "Page 11: 9 articles found\n",
      "Page 12: 2 articles found\n",
      "Page 13: 8 articles found\n",
      "Page 14: 8 articles found\n",
      "Page 15: 6 articles found\n",
      "Page 16: 3 articles found\n",
      "Page 17: 5 articles found\n",
      "Page 18: 8 articles found\n",
      "Page 19: 13 articles found\n",
      "Page 20: 6 articles found\n",
      "Page 21: 14 articles found\n",
      "Property links saved to property_choice_web_links_rent2.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "base_url = 'https://www.pchoicebd.com/property-status/for-rent/'\n",
    "property_links = []\n",
    "\n",
    "# Iterate through pages 1 to 21\n",
    "for page_number in range(1, 22):\n",
    "    if page_number == 1:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = f\"{base_url}page/{page_number}/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all article elements\n",
    "    articles = soup.find_all('article')\n",
    "    print(f\"Page {page_number}: {len(articles)} articles found\")  # Print the number of articles found on each page\n",
    "\n",
    "    # Extract the links from the articles\n",
    "    for article in articles:\n",
    "        link = article.find('a', class_='btn-default')\n",
    "        if link:\n",
    "            property_links.append(link['href'])\n",
    "\n",
    "# Save the links to a CSV file\n",
    "with open('property_choice_web_links_rent2.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['property_links'])\n",
    "\n",
    "    for link in property_links:\n",
    "        csv_writer.writerow([link])\n",
    "\n",
    "print(\"Property links saved to property_choice_web_links_rent2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0ec23",
   "metadata": {},
   "source": [
    "## (for rent properties) This code is for getting the data and including the link of the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dcfa83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to property_choice_website_data_for_rent_with_link.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def find_text_or_none(soup, tag, class_=None, text=None):\n",
    "    \"\"\"Find a tag in the soup that matches the given tag, class, and text,\n",
    "    and return its text. If no such tag is found, return None.\"\"\"\n",
    "    if text:\n",
    "        found_tag = soup.find(tag, class_=class_, text=text)\n",
    "    else:\n",
    "        found_tag = soup.find(tag, class_=class_)\n",
    "    if found_tag:\n",
    "        return found_tag.text.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "property_links = []\n",
    "with open('property_choice_web_links_rent.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader)  # Skip header\n",
    "    for row in csv_reader:\n",
    "        property_links.append(row[0])\n",
    "\n",
    "property_data = []\n",
    "\n",
    "for url in property_links:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    property_title = find_text_or_none(soup, \"h1\", class_=\"entry-title single-property-title\")\n",
    "    price = find_text_or_none(soup, \"span\", class_=\"single-property-price price\")\n",
    "    property_id = find_text_or_none(soup, \"span\", class_=\"meta-item-value\")\n",
    "    area = soup.find(\"span\", class_=\"meta-item-label\", text=\"Area\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Area\") else None\n",
    "    floor = soup.find(\"span\", class_=\"meta-item-label\", text=\"Floor\").find_next(\"span\", class_=\"meta-item-value\", string=True).text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Floor\") else None\n",
    "    bedrooms = soup.find(\"span\", class_=\"meta-item-label\", text=\"Bedrooms\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Bedrooms\") else None\n",
    "    bathrooms = soup.find(\"span\", class_=\"meta-item-label\", text=\"Bathrooms\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Bathrooms\") else None\n",
    "    location = soup.find(\"span\", class_=\"meta-item-label\", text=\"Location\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Location\") else None\n",
    "    status = soup.find(\"span\", class_=\"meta-item-label\", text=\"Status\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Status\") else None\n",
    "    garages = soup.find(\"span\", class_=\"meta-item-label\", text=\"Garages\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Garages\") else None\n",
    "    property_type = soup.find(\"span\", class_=\"meta-item-label\", text=\"Type\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Type\") else None\n",
    "    description = find_text_or_none(soup, \"div\", class_=\"property-content\").strip() if find_text_or_none(soup, \"div\", class_=\"property-content\") else None\n",
    "\n",
    "    latitude = \"latitude_placeholder\"  #\n",
    "    latitude = \"latitude_placeholder\"  # Replace with code to extract latitude\n",
    "    longitude = \"longitude_placeholder\" # Replace with code to extract longitude\n",
    "\n",
    "    features_list = soup.find(\"ul\", class_=\"property-features-list clearfix\")\n",
    "    features = [li.text for li in features_list.find_all(\"li\")] if features_list else []\n",
    "\n",
    "    # Add the link to the property data\n",
    "    property_data.append([url, property_title, price, property_id, area, floor, bedrooms, bathrooms, description, latitude, longitude, features, location, status, garages, property_type])\n",
    "\n",
    "with open('property_choice_website_data_for_rent_with_link.csv', mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Link', 'Title', 'Price', 'Property ID', 'Area', 'Floor', 'Bedrooms', 'Bathrooms', 'Description', 'Latitude', 'Longitude', 'Features', 'Location', 'Status', 'Garages', 'Type'])\n",
    "    for data in property_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(\"Data saved to property_choice_website_data_for_rent_with_link.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef26fc",
   "metadata": {},
   "source": [
    "## (for sale properties) This code is for getting the data and including the link of the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f6b7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to property_choice_web_data_for_sale_with_link.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def find_text_or_none(soup, tag, class_=None, text=None):\n",
    "    \"\"\"Find a tag in the soup that matches the given tag, class, and text,\n",
    "    and return its text. If no such tag is found, return None.\"\"\"\n",
    "    if text:\n",
    "        found_tag = soup.find(tag, class_=class_, text=text)\n",
    "    else:\n",
    "        found_tag = soup.find(tag, class_=class_)\n",
    "    if found_tag:\n",
    "        return found_tag.text.strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "property_links = []\n",
    "with open('property_choice_web_links_sale.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader)  # Skip header\n",
    "    for row in csv_reader:\n",
    "        property_links.append(row[0])\n",
    "\n",
    "property_data = []\n",
    "\n",
    "for url in property_links:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    property_title = find_text_or_none(soup, \"h1\", class_=\"entry-title single-property-title\")\n",
    "    price = find_text_or_none(soup, \"span\", class_=\"single-property-price price\")\n",
    "    property_id = find_text_or_none(soup, \"span\", class_=\"meta-item-value\")\n",
    "    area = soup.find(\"span\", class_=\"meta-item-label\", text=\"Area\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Area\") else None\n",
    "    floor = soup.find(\"span\", class_=\"meta-item-label\", text=\"Floor\").find_next(\"span\", class_=\"meta-item-value\", string=True).text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Floor\") else None\n",
    "    bedrooms = soup.find(\"span\", class_=\"meta-item-label\", text=\"Bedrooms\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Bedrooms\") else None\n",
    "    bathrooms = soup.find(\"span\", class_=\"meta-item-label\", text=\"Bathrooms\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Bathrooms\") else None\n",
    "    location = soup.find(\"span\", class_=\"meta-item-label\", text=\"Location\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Location\") else None\n",
    "    status = soup.find(\"span\", class_=\"meta-item-label\", text=\"Status\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Status\") else None\n",
    "    garages = soup.find(\"span\", class_=\"meta-item-label\", text=\"Garages\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Garages\") else None\n",
    "    property_type = soup.find(\"span\", class_=\"meta-item-label\", text=\"Type\").find_next(\"span\", class_=\"meta-item-value\").text if soup.find(\"span\", class_=\"meta-item-label\", text=\"Type\") else None\n",
    "    description = find_text_or_none(soup, \"div\", class_=\"property-content\").strip() if find_text_or_none(soup, \"div\", class_=\"property-content\") else None\n",
    "\n",
    "    latitude = \"latitude_placeholder\"  #\n",
    "    latitude = \"latitude_placeholder\"  # Replace with code to extract latitude\n",
    "    longitude = \"longitude_placeholder\" # Replace with code to extract longitude\n",
    "\n",
    "    features_list = soup.find(\"ul\", class_=\"property-features-list clearfix\")\n",
    "    features = [li.text for li in features_list.find_all(\"li\")] if features_list else []\n",
    "\n",
    "    # Add the link to the property data\n",
    "    property_data.append([url, property_title, price, property_id, area, floor, bedrooms, bathrooms, description, latitude, longitude, features, location, status, garages, property_type])\n",
    "\n",
    "with open('property_choice_web_data_for_sale_with_link.csv', mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Link', 'Title', 'Price', 'Property ID', 'Area', 'Floor', 'Bedrooms', 'Bathrooms', 'Description', 'Latitude', 'Longitude', 'Features', 'Location', 'Status', 'Garages', 'Type'])\n",
    "    for data in property_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(\"Data saved to property_choice_web_data_for_sale_with_link.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3a991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
